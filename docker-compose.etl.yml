version: '3.8'

services:
  # Scenario 1: AWS Lambda minimum (128MB - free tier default)
  etl-lambda-min:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_lambda_128mb
    mem_limit: 128m
    memswap_limit: 128m
    cpus: 0.5  # Lambda scales CPU with memory
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
    environment:
      - NUM_FILES=20
      - VERBOSE=false
      - CLOUD_PROVIDER=AWS Lambda 128MB
    command: python fetch_db.py
    profiles:
      - lambda-min

  # Scenario 2: AWS Lambda small (256MB - common config)
  etl-lambda-small:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_lambda_256mb
    mem_limit: 256m
    memswap_limit: 256m
    cpus: 0.75
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
    environment:
      - NUM_FILES=50
      - VERBOSE=false
      - CLOUD_PROVIDER=AWS Lambda 256MB
    command: python fetch_db.py
    profiles:
      - lambda-small

  # Scenario 3: AWS Lambda medium (512MB - balanced)
  etl-lambda-medium:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_lambda_512mb
    mem_limit: 512m
    memswap_limit: 512m
    cpus: 1.0
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
    environment:
      - NUM_FILES=100
      - VERBOSE=false
      - CLOUD_PROVIDER=AWS Lambda 512MB
    command: python fetch_db.py
    profiles:
      - lambda-medium

  # Scenario 4: AWS Lambda large (1GB - good performance)
  etl-lambda-large:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_lambda_1gb
    mem_limit: 1g
    memswap_limit: 1g
    cpus: 1.5
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
    environment:
      - NUM_FILES=500
      - VERBOSE=false
      - CLOUD_PROVIDER=AWS Lambda 1GB
    command: python fetch_db.py
    profiles:
      - lambda-large

  # Scenario 5: AWS Lambda XL (2GB - your target!)
  etl-lambda-xl:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_lambda_2gb
    mem_limit: 2g
    memswap_limit: 2g
    cpus: 2.0
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
      - ./output:/app/output  # Mount for CSV output
    environment:
      - NUM_FILES=1000
      - VERBOSE=false
      - CLOUD_PROVIDER=AWS Lambda 2GB
      - SAVE_CSV=false  # Set to 'true' to save CSV
    command: python fetch_db.py
    profiles:
      - lambda-xl

  # Scenario 6: AWS Lambda max common (3GB - high performance)
  etl-lambda-max:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_lambda_3gb
    mem_limit: 3g
    memswap_limit: 3g
    cpus: 2.5
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
    environment:
      - NUM_FILES=2000
      - VERBOSE=false
      - CLOUD_PROVIDER=AWS Lambda 3GB
    command: python fetch_db.py
    profiles:
      - lambda-max

  # Scenario 7: Baseline - no limits (for comparison)
  etl-unlimited:
    build:
      context: .
      dockerfile: Dockerfile.etl
    container_name: etl_unlimited
    volumes:
      - ./fetch_db_docker.py:/app/fetch_db.py:ro
      - /Users/raphaelportela/datasetcovid.zip:/data/datasetcovid.zip:ro
    environment:
      - NUM_FILES=5000
      - VERBOSE=false
      - CLOUD_PROVIDER=Unlimited (baseline)
    command: python fetch_db.py
    profiles:
      - unlimited

