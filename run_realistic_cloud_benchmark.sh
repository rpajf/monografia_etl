#!/bin/bash
# Script para executar benchmarks com limites realistas de cloud
# Testa m√∫ltiplos cen√°rios: 512MB, 1GB, 2GB, 4GB

set -e

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}üöÄ Benchmark Realista de Cloud Environments${NC}"
echo "============================================================"
echo ""
echo -e "${YELLOW}‚ö†Ô∏è  IMPORTANTE:${NC}"
echo "   Este script testa limites REALISTAS de cloud:"
echo "   - 512MB: AWS Lambda b√°sico / Cloud Run m√≠nimo"
echo "   - 1GB:   AWS Lambda comum / Cloud Run m√©dio"
echo "   - 2GB:   AWS Lambda premium / ECS Fargate pequeno"
echo "   - 4GB:   ECS Fargate m√©dio"
echo ""
echo -e "${RED}‚ùå N√ÉO usa 10GB (n√£o √© representativo de recursos limitados!)${NC}"
echo ""

# Check if dataset file exists
DATASET_PATH="${DATASET_PATH:-/Users/raphaelportela/datasetcovid.zip}"
if [ ! -f "$DATASET_PATH" ]; then
    echo -e "${RED}‚ùå Error: Dataset file not found at $DATASET_PATH${NC}"
    echo "   Please set DATASET_PATH environment variable or update the path"
    exit 1
fi

# Build the Docker image
echo ""
echo -e "${BLUE}üì¶ Building Docker image...${NC}"
docker build -f Dockerfile.cloud -t etl-cloud:latest .

# Stop and remove existing containers if they exist
echo ""
echo -e "${BLUE}üßπ Cleaning up existing containers...${NC}"
docker-compose -f docker-compose.cloud.yml down 2>/dev/null || true

# Start PostgreSQL first
echo ""
echo -e "${BLUE}üóÑÔ∏è  Starting PostgreSQL database...${NC}"
docker-compose -f docker-compose.cloud.yml up -d postgres

# Wait for PostgreSQL to be ready
echo ""
echo -e "${BLUE}‚è≥ Waiting for PostgreSQL to be ready...${NC}"
sleep 5

# Get the actual container name
POSTGRES_CONTAINER=$(docker ps --filter "name=postgres" --format "{{.Names}}" | grep -E "(postgres|etl_postgres)" | head -n 1)

if [ -z "$POSTGRES_CONTAINER" ]; then
    POSTGRES_CONTAINER="etl_postgres_cloud"
fi

echo "   Using PostgreSQL container: $POSTGRES_CONTAINER"

until docker exec "$POSTGRES_CONTAINER" pg_isready -U postgres -d etldb > /dev/null 2>&1; do
    echo "   Waiting for PostgreSQL..."
    sleep 2
done
echo -e "${GREEN}‚úÖ PostgreSQL is ready!${NC}"

# Get the actual network name
NETWORK_NAME=$(docker network ls --filter "name=etl_network_cloud" --format "{{.Name}}" | head -n 1)

if [ -z "$NETWORK_NAME" ]; then
    NETWORK_NAME=$(docker network ls --format "{{.Name}}" | grep "etl_network_cloud" | head -n 1)
fi

if [ -z "$NETWORK_NAME" ]; then
    echo -e "${RED}‚ùå Error: Could not find network.${NC}"
    docker network ls
    exit 1
fi

echo "üåê Using network: $NETWORK_NAME"
echo ""

# Define memory limits to test
declare -a MEMORY_LIMITS=("512m" "1g" "2g" "4g")
declare -a CPU_LIMITS=("1.0" "1.5" "2.0" "2.0")
declare -a SCENARIOS=("AWS Lambda B√°sico" "AWS Lambda Comum" "AWS Lambda Premium" "ECS Fargate M√©dio")

# Ask user which scenarios to run
echo -e "${YELLOW}Escolha os cen√°rios para testar:${NC}"
echo "1) Todos os cen√°rios (512MB, 1GB, 2GB, 4GB)"
echo "2) Apenas Serverless (512MB, 1GB, 2GB)"
echo "3) Apenas 1GB e 2GB (mais representativos)"
echo "4) Apenas 2GB (serverless premium)"
echo "5) Customizado"
echo ""
read -p "Escolha (1-5): " choice

case $choice in
    1)
        SELECTED_INDICES=(0 1 2 3)
        ;;
    2)
        SELECTED_INDICES=(0 1 2)
        ;;
    3)
        SELECTED_INDICES=(1 2)
        ;;
    4)
        SELECTED_INDICES=(2)
        ;;
    5)
        echo ""
        echo "Escolha os √≠ndices (0=512MB, 1=1GB, 2=2GB, 3=4GB):"
        read -p "√çndices (separados por espa√ßo): " indices
        SELECTED_INDICES=($indices)
        ;;
    *)
        echo -e "${RED}Op√ß√£o inv√°lida. Executando todos os cen√°rios.${NC}"
        SELECTED_INDICES=(0 1 2 3)
        ;;
esac

echo ""
echo -e "${GREEN}üìä Executando ${#SELECTED_INDICES[@]} cen√°rio(s)...${NC}"
echo ""

# Create results directory
RESULTS_DIR="cloud_benchmark_results_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$RESULTS_DIR"

# Run benchmarks for each selected scenario
for idx in "${SELECTED_INDICES[@]}"; do
    MEM_LIMIT="${MEMORY_LIMITS[$idx]}"
    CPU_LIMIT="${CPU_LIMITS[$idx]}"
    SCENARIO="${SCENARIOS[$idx]}"
    
    echo ""
    echo "============================================================"
    echo -e "${BLUE}üîµ Cen√°rio: $SCENARIO${NC}"
    echo -e "${BLUE}   Mem√≥ria: $MEM_LIMIT | CPU: $CPU_LIMIT${NC}"
    echo "============================================================"
    echo ""
    
    # Create scenario-specific result directories
    SCENARIO_DIR="$RESULTS_DIR/${MEM_LIMIT}"
    mkdir -p "$SCENARIO_DIR/sync_result"
    mkdir -p "$SCENARIO_DIR/async_result"
    mkdir -p "$SCENARIO_DIR/output"
    
    # Run the ETL application with memory limit
    docker run --rm \
        --name "etl_app_cloud_${MEM_LIMIT}" \
        --memory="$MEM_LIMIT" \
        --memory-swap="$MEM_LIMIT" \
        --cpus="$CPU_LIMIT" \
        --network "$NETWORK_NAME" \
        -e DB_HOST=postgres \
        -e DB_PORT=5432 \
        -e DB_NAME=etldb \
        -e DB_USER=postgres \
        -e DB_PASSWORD="" \
        -e DATASET_PATH=/data/datasetcovid.zip \
        -e MEMORY_LIMIT="$MEM_LIMIT" \
        -e SCENARIO_NAME="$SCENARIO" \
        -v "$(pwd):/app" \
        -v "$DATASET_PATH:/data/datasetcovid.zip:ro" \
        -v "$(pwd)/$SCENARIO_DIR/sync_result:/app/sync_result" \
        -v "$(pwd)/$SCENARIO_DIR/async_result:/app/async_result" \
        -v "$(pwd)/$SCENARIO_DIR/output:/app/output" \
        etl-cloud:latest python main.py || {
            echo -e "${RED}‚ùå Erro ao executar com $MEM_LIMIT${NC}"
            echo "   Poss√≠vel OOM (Out of Memory) ou outro erro"
            continue
        }
    
    echo ""
    echo -e "${GREEN}‚úÖ Cen√°rio $SCENARIO ($MEM_LIMIT) conclu√≠do!${NC}"
    echo "   Resultados salvos em: $SCENARIO_DIR/"
done

echo ""
echo "============================================================"
echo -e "${GREEN}‚úÖ Todos os benchmarks conclu√≠dos!${NC}"
echo "============================================================"
echo ""
echo -e "${BLUE}üìä Resultados salvos em: $RESULTS_DIR/${NC}"
echo ""
echo "Estrutura:"
echo "  $RESULTS_DIR/"
for idx in "${SELECTED_INDICES[@]}"; do
    MEM_LIMIT="${MEMORY_LIMITS[$idx]}"
    echo "    ‚îú‚îÄ‚îÄ $MEM_LIMIT/"
    echo "    ‚îÇ   ‚îú‚îÄ‚îÄ sync_result/    (gr√°ficos benchmark s√≠ncrono)"
    echo "    ‚îÇ   ‚îú‚îÄ‚îÄ async_result/   (gr√°ficos benchmark ass√≠ncrono)"
    echo "    ‚îÇ   ‚îî‚îÄ‚îÄ output/        (outros outputs)"
done
echo ""
echo -e "${YELLOW}üí° Dica: Compare os resultados para analisar trade-offs de mem√≥ria vs performance${NC}"
echo ""

# Optional: Keep PostgreSQL running for inspection
read -p "Parar PostgreSQL? (s/N): " stop_postgres
if [[ "$stop_postgres" =~ ^[Ss]$ ]]; then
    docker-compose -f docker-compose.cloud.yml down
    echo -e "${GREEN}‚úÖ PostgreSQL parado.${NC}"
else
    echo -e "${BLUE}‚ÑπÔ∏è  PostgreSQL continua rodando para inspe√ß√£o.${NC}"
fi


